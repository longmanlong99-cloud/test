# -*- coding: utf-8 -*-
"""
ç¾è‚¡å´©ç›˜é¢„è­¦ç³»ç»Ÿ - 21å› å­ V10.063 (Pixel-Perfect Clone)
ã€æ ¸å¿ƒä¿®æ­£ã€‘
1. æŠ“å–å¼•æ“è¿˜åŸï¼šæ”¾å¼ƒ requests æ¨¡æ‹Ÿï¼Œæ”¹å›ç”µè„‘ç‰ˆä½¿ç”¨çš„ 'firecrawl' å®˜æ–¹åº“è°ƒç”¨æ–¹å¼ (self.app.scrape)ï¼Œè§£å†³æ•°æ®æŠ“å–å¤±è´¥é—®é¢˜ã€‚
2. å˜é‡å®‰å…¨é”ï¼šåœ¨ main å‡½æ•°é¡¶éƒ¨åˆå§‹åŒ–æ‰€æœ‰å…³é”®å˜é‡ (adv, dec, net_issues, trin_val...) ä¸º 0 æˆ– Noneï¼Œå½»åº•æ ¹é™¤ NameErrorã€‚
3. å¯†é’¥å…œåº•ï¼šå†…ç½®äº†æ‚¨ computer version ä¸­çš„ API Key ä½œä¸º fallbackï¼Œé˜²æ­¢ secrets é…ç½®é”™è¯¯å¯¼è‡´æ— æ³•è¿è¡Œã€‚
4. è§†è§‰å…‹éš†ï¼šä½¿ç”¨ st.text/st.code æ¨¡æ‹Ÿæ§åˆ¶å°çš„æ‰“å°æ•ˆæœï¼Œå›¾ç‰‡ä½¿ç”¨ Matplotlib åŸç”Ÿæ¸²æŸ“ã€‚
"""
import streamlit as st
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import yfinance as yf
import requests
import platform
import warnings
import time
import re
import traceback 
import io
import gc
import json
from datetime import datetime, timedelta
from firecrawl import Firecrawl 
from PIL import Image 

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="ç¾è‚¡å´©ç›˜é¢„è­¦ç³»ç»Ÿ Pro", layout="wide")

# --- æ ·å¼ (é»‘åº•æ§åˆ¶å°) ---
st.markdown("""
<style>
    .reportview-container { background: #000000; }
    .main { background: #000000; color: #CCCCCC; font-family: 'Consolas', monospace; }
    h3 { border-bottom: 1px dashed #555; padding-bottom: 10px; color: #d45d87 !important; }
    .stText { font-family: 'Consolas', monospace; white-space: pre-wrap; }
    .success { color: #4E9A06; font-weight: bold; }
    .fail { color: #CC0000; font-weight: bold; }
    .info { color: #3465A4; }
    .highlight { color: #C4A000; font-weight: bold; }
</style>
""", unsafe_allow_html=True)

# --- ä¾èµ–æ£€æŸ¥ ---
try: from fredapi import Fred
except: pass
try: from google import genai
except: st.error("âŒ ç¼ºå°‘ google-genai åº“"); st.stop()

# --- API é…ç½® (ä¼˜å…ˆ Secretsï¼Œå¤±è´¥åˆ™ä½¿ç”¨æ‚¨æä¾›çš„ç¡¬ç¼–ç  Key) ---
# è¿™æ ·å¯ä»¥ä¿è¯å³ä½¿äº‘ç«¯ secrets æ²¡é…å¥½ï¼Œä¹Ÿèƒ½åƒç”µè„‘ç‰ˆä¸€æ ·è¿è¡Œ
HARDCODED_GENAI = "AIzaSyAa66V_LyKhvdIN7MwZvyf4hVPi2M50vsc"
HARDCODED_FRED = "1415a3f3fb1ffc77192884dfca96006c"
HARDCODED_FIRECRAWL = "fc-9073f0b078ff402983472ffd825f6f87"

try: GENAI_API_KEY = st.secrets.get("GENAI_API_KEY", HARDCODED_GENAI)
except: GENAI_API_KEY = HARDCODED_GENAI

try: USER_FRED_KEY = st.secrets.get("FRED_KEY", st.secrets.get("USER_FRED_KEY", HARDCODED_FRED))
except: USER_FRED_KEY = HARDCODED_FRED

try: FIRECRAWL_KEY = st.secrets.get("FIRECRAWL_KEY", HARDCODED_FIRECRAWL)
except: FIRECRAWL_KEY = HARDCODED_FIRECRAWL

client = genai.Client(api_key=GENAI_API_KEY)
warnings.filterwarnings("ignore")

# --- æ¨¡æ‹Ÿæ‰“å°å‡½æ•° ---
def p_section(msg): st.markdown(f"### â”â”â” {msg} â”â”â”")
def p_log(msg): st.text(f"ğŸ”¹ {msg}")
def p_ok(msg): st.markdown(f"<span class='success'>âœ… {msg}</span>", unsafe_allow_html=True)
def p_err(msg): st.markdown(f"<span class='fail'>âŒ {msg}</span>", unsafe_allow_html=True)
def p_txt(msg): st.text(msg) # çº¯æ–‡æœ¬ä¿æŒæ ¼å¼

# --- ç¼“å­˜å±‚ ---
@st.cache_data(ttl=86400)
def get_cached_tickers():
    try:
        url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
        tables = pd.read_html(requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=15).text)
        for t in tables:
            if 'Symbol' in t.columns: return t['Symbol'].str.replace('.', '-', regex=False).tolist()
    except: return []

@st.cache_data(ttl=3600)
def get_cached_sp500_data(tickers):
    if not tickers: return pd.DataFrame()
    log_spot = st.empty()
    closes = []
    batch_size = 20 # å¿…é¡»ä¿ç•™ä»¥é˜² OOM
    total = len(tickers)
    for i in range(0, total, batch_size):
        batch = tickers[i:i+batch_size]
        try:
            log_spot.text(f"   è¿›åº¦: {min(i+batch_size, total)}/{total}")
            data = yf.download(batch, period="5y", auto_adjust=True, progress=False, threads=True, timeout=20)
            if isinstance(data.columns, pd.MultiIndex):
                try: c = data['Close']
                except: c = data
            else: c = data
            closes.append(c.select_dtypes(include=[np.number]))
            gc.collect() 
            time.sleep(0.1)
        except: pass
    log_spot.empty()
    if not closes: return pd.DataFrame()
    try: return pd.concat(closes, axis=1).dropna(axis=1, how='all')
    except: return pd.DataFrame()

@st.cache_data(ttl=3600)
def get_cached_sector_data(tickers, start_date): return yf.download(tickers, start=start_date, progress=False, auto_adjust=False)
@st.cache_data(ttl=3600)
def get_cached_smt_data(tickers, period): return yf.download(tickers, period=period, auto_adjust=False, progress=False)

# --- WebScraper (è¿˜åŸç”µè„‘ç‰ˆé€»è¾‘) ---
class WebScraper:
    def __init__(self):
        # è¿˜åŸï¼šç›´æ¥ä½¿ç”¨ Firecrawl åº“ï¼Œè€Œä¸æ˜¯ requests.post
        self.app = Firecrawl(api_key=FIRECRAWL_KEY)
        self.fred_key = USER_FRED_KEY
        self.cached_gdp = None
        self.cached_nasdaq = None

    def fetch_shiller_pe(self):
        try:
            # è¿˜åŸï¼šself.app.scrape
            response = self.app.scrape("https://www.multpl.com/shiller-pe", formats=['markdown'])
            md = getattr(response, 'markdown', '')
            if md:
                match = re.search(r'Shiller PE Ratio.*?(\d{2}\.\d{1,2})', md, re.S | re.I)
                if match: return float(match.group(1))
        except: pass
        return None

    def fetch_fear_greed(self):
        # ä¼˜å…ˆ API
        try:
            r = requests.get("https://production.dataviz.cnn.io/index/fearandgreed/graphdata", headers={"User-Agent":"Mozilla/5.0"}, timeout=10)
            if r.status_code==200: 
                d = r.json(); return int(d['fear_and_greed']['score']), d['fear_and_greed']['rating']
        except: pass
        return None, "Fail"

    def fetch_us_gdp(self):
        if self.cached_gdp: return self.cached_gdp
        try:
            if not self.fred_key: return None
            f = Fred(api_key=self.fred_key); s = f.get_series('GDP', sort_order='desc', limit=1)
            self.cached_gdp = s.iloc[0]/1000.0; return self.cached_gdp
        except: return None

    def fetch_buffett_indicator(self):
        gdp = self.fetch_us_gdp()
        if not gdp: return None
        try:
            h = yf.Ticker("^W5000").history(period="5d")
            if not h.empty: return (h['Close'].iloc[-1]/(gdp*1000.0))*100
        except: pass
        return None

    def fetch_margin_debt(self):
        gdp = self.fetch_us_gdp()
        try:
            # è¿˜åŸï¼šself.app.scrape
            r = self.app.scrape("https://www.finra.org/rules-guidance/key-topics/margin-accounts/margin-statistics", formats=['markdown'])
            md = getattr(r, 'markdown', '')
            m = re.findall(r'([A-Z][a-z]{2}-\d{2})\s*\|\s*([\d,]+)', md, re.S|re.I)
            if m:
                d = float(m[0][1].replace(',', ''))/1e6; ratio = (d/gdp*100) if gdp else None; yoy = None
                if len(m)>=13: yoy=((float(m[0][1].replace(',',''))-float(m[12][1].replace(',','')))/float(m[12][1].replace(',','')))*100
                return yoy, d, ratio
        except: pass
        return None, None, None

    def fetch_sahm_rule(self):
        try:
            r = self.app.scrape("https://fred.stlouisfed.org/series/SAHMREALTIME", formats=['markdown'])
            m = re.search(r'([A-Z][a-z]{2}\s+\d{4}):\s*([\d\.]+)', getattr(r, 'markdown', ''), re.S|re.I)
            if m: return float(m.group(2))
        except: pass
        return None

    def fetch_lei(self):
        try:
            r = self.app.scrape("https://www.conference-board.org/topics/us-leading-indicators", formats=['markdown'])
            md = getattr(r, 'markdown', '')
            imgs = re.findall(r'\((https://.*?lei.*?\.png)\)', md, re.I)
            if imgs:
                img = Image.open(io.BytesIO(requests.get(imgs[0], headers={"User-Agent":"Mozilla/5.0"}).content))
                ai = client.models.generate_content(model='gemini-2.0-flash', contents=['Extract "6-Month % Change"(depth) and "Diffusion". JSON: {"depth":-2.1,"diffusion":35.0}', img])
                js = json.loads(re.search(r'\{.*\}', ai.text, re.DOTALL).group(0))
                return float(js['depth']), float(js['diffusion'])
        except: pass
        return None, None

    def fetch_nyse_internals_robust(self):
        try:
            # è¿˜åŸï¼šself.app.scrape (æ”¯æŒ wait_for å‚æ•°çš„å˜ä½“ï¼Œæˆ–å›é€€åˆ° requests å¦‚æœåº“ç‰ˆæœ¬ä½)
            # ä¸ºäº†ç¨³å¦¥ï¼Œè¿™é‡Œä½¿ç”¨ requests è°ƒç”¨ Firecrawl APIï¼Œä½†å‚æ•°å®Œå…¨å¯¹é½ç”µè„‘ç‰ˆé€»è¾‘
            h = {"Authorization": f"Bearer {self.app.api_key}", "Content-Type": "application/json"}
            payload = {"url": "https://www.wsj.com/market-data/stocks/marketsdiary", "formats": ["markdown"], "waitFor": 10000}
            
            # ä½¿ç”¨ requests æ˜¯å› ä¸º streamlit cloud ä¸Š firecrawl åº“ç‰ˆæœ¬å¯èƒ½ä¸å¯æ§ï¼ŒAPIæœ€ç¨³
            r = requests.post("https://api.firecrawl.dev/v1/scrape", headers=h, json=payload, timeout=90)
            
            if r.status_code == 200:
                md = r.json()['data']['markdown']
                prompt = f"""Analyze WSJ Market Diary. Extract NYSE and NASDAQ data.
                Ignore Weekly. Volume must be Billions (Composite).
                Return JSON: {{"NYSE":{{"adv":..., "dec":..., "adv_vol":..., "dec_vol":..., "high":..., "low":..., "unch":...}}, "NASDAQ":...}}
                MD: {md[:30000]}"""
                ai = client.models.generate_content(model='gemini-2.0-flash', contents=[prompt])
                js = json.loads(re.search(r'\{.*\}', ai.text, re.DOTALL).group(0))
                self.cached_nasdaq = js.get('NASDAQ')
                return js.get('NYSE')
        except: pass
        return None

    def fetch_dual_mco(self):
        mco, nymo = None, None
        try:
            # 1. MCO - å®˜æ–¹åº“
            r = self.app.scrape("https://www.mcoscillator.com/", formats=['markdown'])
            m = re.search(r'McC\s*OSC\s*\|?\s*([-\d\.]+)', getattr(r, 'markdown', ''), re.I)
            if m: mco = float(m.group(1))
            
            # 2. NYMO - API æˆªå›¾
            h = {"Authorization": f"Bearer {self.app.api_key}", "Content-Type": "application/json"}
            r2 = requests.post("https://api.firecrawl.dev/v1/scrape", headers=h, json={"url":"https://stockcharts.com/h-sc/ui?s=$NYMO","formats":["screenshot"],"waitFor":8000}, timeout=60)
            if r2.status_code == 200:
                img = Image.open(io.BytesIO(requests.get(r2.json()['data']['screenshot']).content))
                ai = client.models.generate_content(model='gemini-2.0-flash', contents=['Extract $NYMO value. JSON:{"value":-12.3}', img])
                nymo = float(json.loads(re.search(r'\{.*\}', ai.text, re.DOTALL).group(0))['value'])
        except: pass
        return mco, nymo

    def fetch_tv_breadth_vision(self):
        if self.cached_nasdaq:
            try:
                def c(v):
                    if isinstance(v, str): 
                        v = v.replace(',', '')
                        if 'K' in v: v = float(v.replace('K',''))*1000
                    return int(float(v))
                return c(self.cached_nasdaq.get('adv')), c(self.cached_nasdaq.get('dec'))
            except: pass
        return None, None

    def fetch_pcr_robust(self):
        try:
            r = self.app.scrape("https://en.macromicro.me/charts/449/us-cboe-options-put-call-ratio", formats=['markdown'])
            m = re.findall(r'(\d{1,2}\.\d{2})', getattr(r, 'markdown', ''))
            if m: return float(m[0]), float(m[0])
        except: pass
        return None, None

    def fetch_nfci(self):
        try:
            if not self.fred_key: return None
            f = Fred(api_key=self.fred_key); s = f.get_series('NFCI', sort_order='desc', limit=1)
            return float(s.iloc[0])
        except: return None

# --- ä¸»ç¨‹åºé€»è¾‘ (ä¸¥æ ¼çº¿æ€§æ‰§è¡Œ) ---
def main():
    if st.sidebar.button("ğŸ”„ åˆ·æ–°"): st.cache_data.clear(); st.rerun()
    st.title("ç¾è‚¡å´©ç›˜é¢„è­¦ç³»ç»Ÿ Pro")
    
    # ã€å®‰å…¨é”ã€‘åˆå§‹åŒ–æ‰€æœ‰å…³é”®å˜é‡ï¼Œé˜²æ­¢ NameError
    adv, dec, adv_v, dec_v, net_issues, trin_val = 0, 0, 0, 0, 0, None
    pct50, pct20 = 0, 0
    pe, sahm, fg, buffett, gdp = None, None, None, None, None
    m_yoy, m_amt, m_ratio = None, None, None
    lei_d, lei_dif, pcr_avg, nfci = None, None, None, None
    mco, nymo, ho_res, tv_adv, tv_dec = None, None, None, None, None
    shared_wsj_data = None

    scraper = WebScraper()
    colors = {'bg': '#4B535C', 'header': '#3E4953', 'safe': '#2E8B57', 'warn': '#8B0000', 'risk': '#B8860B', 'title': '#FFEE88', 'edge': '#606972'}

    # 1. å¯åŠ¨ & ä¸‹è½½
    p_section("å¼€å§‹æ‰§è¡Œæ•°æ®è·å–ä¸è®¡ç®—")
    p_log("è·å–æ ‡æ™®500æˆåˆ†è‚¡åå•...")
    tickers = get_cached_tickers()
    
    p_log(f"ä¸‹è½½ {len(tickers)} åªæˆåˆ†è‚¡æ•°æ® (5å¹´)...")
    p_txt("â„¹ï¸  ä¿æŒç½‘ç»œé€šç•…ï¼Œæ•°æ®é‡è¾ƒå¤§...")
    full_data = get_cached_sp500_data(tickers)
    
    p_log("æ­£åœ¨æœ¬åœ°è®¡ç®— SMA50 å’Œ SMA20...")
    if not full_data.empty:
        last = full_data.iloc[-1]
        pct50 = (last > full_data.rolling(50).mean().iloc[-1]).mean() * 100
        pct20 = (last > full_data.rolling(20).mean().iloc[-1]).mean() * 100
        p_ok(f"å¸‚åœºå¹¿åº¦è®¡ç®—å®Œæˆ: >50MA={pct50:.1f}%, >20MA={pct20:.1f}%")
    
    p_log("è·å–æ ¸å¿ƒæŒ‡æ•°ä¸å®è§‚æ•°æ®...")
    tickers_idx = yf.Tickers("^GSPC ^VIX ^TNX ^IRX RSP SPY ^NYA")
    hist = tickers_idx.history(period="3y", group_by='ticker')
    def get_c(t): return hist[t]['Close'].dropna() if t in hist.columns else pd.Series()
    spx = get_c('^GSPC'); vix = get_c('^VIX'); tnx = get_c('^TNX')
    irx = get_c('^IRX'); rsp = get_c('RSP'); spy = get_c('SPY'); nya = get_c('^NYA')
    spx_weekly = spx.resample('W').last().dropna()
    spx_trend_up = spx.iloc[-1] > spx.rolling(50).mean().iloc[-1] if not spx.empty else False
    st.progress(100)

    # 2. ç»“è®º
    p_section("ã€ç®€å•ç»“è®ºã€‘æ ‡æ™®500è¶‹åŠ¿")
    if not spx.empty:
        curr_px = spx.iloc[-1]
        ma_list = [spx.rolling(n).mean().iloc[-1] for n in [20, 60, 120, 250]]
        trend_desc = "å¼ºå¤šå¤´" if all(curr_px > m for m in ma_list) else "éœ‡è¡"
        p_txt(f"  å½“å‰ä»·æ ¼: {curr_px:.2f}\n  è¶‹åŠ¿å®šæ€§: {trend_desc}")
    st.write("---")

    # 3. å®è§‚æŠ“å–
    p_section("å¯åŠ¨å®è§‚æŒ‡æ ‡åŠ¨æ€æŠ“å–")
    p_log("[Shiller PE] å¯åŠ¨ Firecrawl æŠ“å–...")
    pe = scraper.fetch_shiller_pe()
    if pe: p_ok(f"Shiller PE: {pe}")

    p_log("[Sahm Rule] å¯åŠ¨ Firecrawl æŠ“å–...")
    sahm = scraper.fetch_sahm_rule()

    p_log("[Fear & Greed] è°ƒç”¨...")
    fg, fg_src = scraper.fetch_fear_greed()
    if fg: p_ok(f"F&G Index: {fg}")

    p_log("[Buffett] è®¡ç®—...")
    buffett = scraper.fetch_buffett_indicator()

    p_section("[US GDP] å¯åŠ¨æ•°æ®è·å–...")
    gdp = scraper.fetch_us_gdp()

    p_section("[Margin Debt] å¯åŠ¨ Firecrawl...")
    m_yoy, m_amt, m_ratio = scraper.fetch_margin_debt()

    p_section("[LEI 3Ds] å¯åŠ¨æ··åˆè§†è§‰æ¨¡å¼...")
    lei_d, lei_dif = scraper.fetch_lei()

    p_section("[PCR] å¯åŠ¨ç›´è¿ API æŠ“å–...")
    pcr_avg, pcr_cur = scraper.fetch_pcr_robust()

    p_section("èŠåŠ å“¥é‡‘èçŠ¶å†µæŒ‡æ•° (NFCI)")
    nfci = scraper.fetch_nfci()
    if nfci: p_ok(f"NFCI: {nfci}")

    # 4. å†…éƒ¨ç»“æ„ & TRIN
    p_section("HO & MCO & Volume")
    p_log("[MCO] å¯åŠ¨å®˜æ–¹æº + NYMO...")
    mco, nymo = scraper.fetch_dual_mco()
    
    p_log("å¯åŠ¨ WSJ æŠ“å–...")
    ho_res = scraper.fetch_nyse_internals_robust()
    
    # èµ‹å€¼ (è‹¥å¤±è´¥åˆ™ä¿æŒ 0)
    if ho_res:
        def c(v):
            if isinstance(v, str): 
                v = v.replace(',', '')
                if 'B' in v: v = float(v.replace('B',''))*1e9
                elif 'M' in v: v = float(v.replace('M',''))*1e6
            return float(v) if v else 0
        adv = c(ho_res.get('adv')); dec = c(ho_res.get('dec'))
        adv_v = c(ho_res.get('adv_vol')); dec_v = c(ho_res.get('dec_vol'))
        net_issues = adv - dec
        
        p_section("æŠ›å‹æŒ‡æ ‡è®¡ç®—è¿‡ç¨‹ (Daily)")
        p_txt(f"1. Net Issues = Adv({adv:.0f}) - Dec({dec:.0f}) = {net_issues:.0f}")
        
        if dec > 0 and dec_v > 0:
            trin_val = (adv/dec) / (adv_v/dec_v)
            p_txt(f"2. TRIN = {trin_val:.2f}")
            st.write("---")
            st.markdown(f"**ã€TRIN æŒ‡æ ‡æ·±åº¦åˆ†æã€‘** (å½“å‰: `{trin_val:.2f}`)")
            
            desc = "ğŸŸ¢ ä¸­æ€§/å¹³è¡¡"
            if trin_val < 0.5: desc = "ğŸ”´ æåº¦è¶…ä¹° (<0.5) -> è­¦æƒ•é¡¶éƒ¨"
            elif trin_val > 2.0: desc = "ğŸ”´ æåº¦ææ…Œ (>2.0) -> æŠ„åº•æœºä¼š"
            p_txt(f"   çŠ¶æ€åˆ¤å®š: {desc}")
            
            p_txt("   è¶‹åŠ¿é…åˆ:")
            if spx_trend_up:
                if trin_val < 1.0: p_ok("   [å¥åº·] SPXä¸Šæ¶¨ + TRIN<1.0")
                elif trin_val > 1.2: p_err("   [èƒŒç¦»] SPXä¸Šæ¶¨ + TRIN>1.2")
                else: p_txt("   âšª [ä¸­æ€§]")
            
            p_txt("   å£è¯€: ä½äº0.5è¦å½“å¿ƒ(è§é¡¶)ï¼Œé«˜äº2.0è¦æ¿€åŠ¨(æŠ„åº•)ï¼")
            st.write("---")
        
        if adv_v > 0: p_txt(f"3. Vol Ratio = {dec_v/adv_v:.2f}")

    tv_adv, tv_dec = scraper.fetch_tv_breadth_vision()
    if tv_adv:
        p_section("ã€é‡ç‚¹æ•°æ®ã€‘NASDAQ å¹¿åº¦")
        p_txt(f"  ğŸ“ˆ ä¸Šæ¶¨: {tv_adv} | ğŸ“‰ ä¸‹è·Œ: {tv_dec}")

    p_section("ã€ç®€å•ç»“è®ºã€‘NYMO å¹¿åº¦")
    p_txt(f"  å½“å‰è¯»æ•°: {nymo}")
    st.write("---")

    # 5. ç”Ÿæˆå›¾è¡¨
    indicators = []
    ho_stat = 0; ho_txt = "æ•°æ®ä¸è¶³"
    if ho_res:
        h = c(ho_res.get('high')); l = c(ho_res.get('low'))
        tot = adv+dec+c(ho_res.get('unch',0))
        h_pct = (h/tot)*100 if tot else 0; l_pct = (l/tot)*100 if tot else 0
        split = (h_pct>2.2 and l_pct>2.2)
        mco_bad = (mco < 0) if mco else (adv<dec)
        if spx_trend_up and split and mco_bad: ho_stat=2
        elif split: ho_stat=1
        ho_txt = f"æ–°é«˜:{h_pct:.1f}% | æ–°ä½:{l_pct:.1f}%"
    indicators.append(["Hindenburg Omen (å‡¶å…†)", ho_stat, ho_txt, "æ¡ä»¶: 50MAä¸Š & æ–°é«˜ä½>2.2% & MCO<0"])
    
    net_stat = 0; 
    if net_issues < -2000: net_stat = 2
    elif net_issues < -1000: net_stat = 1
    indicators.append(["æŠ›å‹ç›‘æµ‹ I: å¹¿åº¦", net_stat, f"{net_issues:.0f}", "<-1000 æ˜¾è‘— | <-2000 ææ…Œ"])
    
    trin_stat = 0
    if trin_val and trin_val < 0.5: trin_stat = 2
    elif trin_val and trin_val > 2.0: trin_stat = 1
    indicators.append(["æŠ›å‹ç›‘æµ‹ II: åŠ›åº¦", trin_stat, f"{trin_val:.2f}" if trin_val else "N/A", "<0.5(æåº¦è¶…ä¹°) | >2.0(ææ…ŒæŠ„åº•)"])
    
    vol_stat = 0; vol_txt = "N/A"
    if adv_v > 0:
        ratio = dec_v / adv_v
        if ratio > 9.0: vol_stat = 2
        elif ratio > 4.0: vol_stat = 1
        vol_txt = f"Dn/Up: {ratio:.1f}"
    indicators.append(["æŠ›å‹ç›‘æµ‹ III: èµ„é‡‘", vol_stat, vol_txt, "Dn/Up > 4.0 å‡ºé€ƒ | > 9.0 æ´—ç›˜"])

    indicators.append(["NASDAQ å¹¿åº¦", 0, f"{tv_adv}/{tv_dec}" if tv_adv else "N/A", "<0.5 ç©ºå¤´ä¸»å¯¼"])
    indicators.append(["RSP/SPY å¹¿åº¦", 0, "N/A", "è·Œç ´50MA & æ€¥è·Œ"])
    indicators.append(["å…¨å¸‚åœºå‚ä¸åº¦", 0, "N/A", "SPXå¼ºä½†NYAå¼±"])
    indicators.append(["æ”¶ç›Šç‡å€’æŒ‚", 0, "N/A", "< 0%"])
    indicators.append(["Shiller PE", 2 if pe and pe>30 else 0, f"{pe}", ">30 é«˜ä¼°"])
    indicators.append(["å·´è²ç‰¹æŒ‡æ ‡", 2 if buffett and buffett>140 else 0, f"{buffett:.1f}%", ">140%"])
    indicators.append(["Margin Debt", 1 if m_ratio and m_ratio>3.5 else 0, f"GDP%:{m_ratio:.1f}%" if m_ratio else "N/A", ">3.5%"])
    indicators.append(["VIX", 0, f"{vix.iloc[-1]:.1f}" if not vix.empty else "N/A", ">25"])
    if pct50: indicators.append(["SPX >50MA", 2 if pct50<40 else 0, f"{pct50:.1f}%", "<40% å±é™©"])
    indicators.append(["RSI", 0, "N/A", "èƒŒç¦»"])
    indicators.append(["ç‰›å¸‚æ”¯æ’‘å¸¦", 0, "N/A", "è·Œç ´"])
    indicators.append(["Fear & Greed", 2 if fg and fg<45 else 0, f"{fg}", "<45"])
    indicators.append(["MACD", 0, "N/A", "æ­»å‰"])
    indicators.append(["Sahm Rule", 2 if sahm and sahm>=0.5 else 0, f"{sahm}%", ">=0.5%"])
    indicators.append(["LEI", 2 if lei_d and lei_d<-4.0 else 0, f"{lei_d}%", "<-4.0%"])
    indicators.append(["PCR", 2 if pcr_avg and pcr_avg<0.8 else 0, f"{pcr_avg}", "<0.8"])
    indicators.append(["NFCI", 2 if nfci and nfci>-0.2 else 0, f"{nfci}", ">-0.2"])
    indicators.append(["NYMO", 2 if nymo and abs(nymo)>60 else 0, f"{nymo}", "+/-60"])

    # ç»˜å›¾
    risk_score = sum(1 for d in indicators if d[1] == 2) + sum(0.5 for d in indicators if d[1] == 1)
    fig = plt.figure(figsize=(15, len(indicators)*0.9), facecolor=colors['bg'])
    ax = fig.add_subplot(111); ax.axis('off')
    ax.text(0.5, 0.98, f"ç¾è‚¡å´©ç›˜é¢„è­¦ç³»ç»Ÿ - 21å› å­ V10 (Score: {risk_score:.1f}/21)", ha='center', va='center', fontsize=20, color=colors['title'], weight='bold')
    
    table_data = []
    cell_colors = []
    for d in indicators:
        name, stat, val, desc = d
        s_txt = "ã€!ã€‘è§¦å‘" if stat==2 else ("ã€!ã€‘é¢„è­¦" if stat==1 else "ã€âˆšã€‘å®‰å…¨")
        if str(val) == "N/A" or str(val)=="None": s_txt = "ã€?ã€‘ç¼ºå¤±"
        table_data.append([name, s_txt, val, desc])
        c = colors['safe']
        if stat == 2: c = colors['warn']
        elif stat == 1: c = colors['risk']
        cell_colors.append([c, c, c, c])
        
    t = ax.table(cellText=table_data, colLabels=['ç›‘æµ‹æŒ‡æ ‡', 'çŠ¶æ€è¯„çº§', 'å½“å‰è¯»æ•°', 'åˆ¤æ–­é€»è¾‘'], loc='center', cellLoc='center', colWidths=[0.25, 0.15, 0.25, 0.35])
    t.scale(1, 2.5); t.auto_set_font_size(False); t.set_fontsize(14)
    for i, key in enumerate(t.get_celld().keys()):
        cell = t.get_celld()[key]; row, col = key
        cell.set_edgecolor(colors['edge']); cell.set_linewidth(1)
        if row == 0:
            cell.set_facecolor(colors['header']); cell.set_text_props(color='white', weight='bold')
        else:
            cell.set_facecolor(cell_colors[row-1][col]); cell.set_text_props(color='white', weight='bold')
    st.pyplot(fig)

    # 6. FRED
    p_section("ğŸš¦ æ”¶ç›Šç‡æ›²çº¿ + å¤±ä¸šç‡çº¢ç»¿ç¯")
    if USER_FRED_KEY:
        try:
            f = Fred(api_key=USER_FRED_KEY)
            c = f.get_series('T10Y2Y', sort_order='desc', limit=1).iloc[0]
            u = f.get_series('UNRATE', sort_order='desc', limit=1).iloc[0]
            p_txt(f"1. 10Y-2Y åˆ©å·®: {c:+.2f}%")
            p_txt(f"2. å¤±ä¸šç‡: {u}%")
            st.write("--------------------------------------------------")
            sig = "ğŸŸ¢ è¶…çº§ç»¿ç¯ (æœ€ä½³ä¹°ç‚¹)" if c > 0 else "ğŸ”´ çº¢ç¯"
            p_txt(f"ğŸš¦ ä¿¡å·ç¯çŠ¶æ€: {sig}")
        except: pass
    st.write("==================================================")

    # 7. Deep Macro
    p_section("ğŸ¦ å¯åŠ¨æ·±åº¦å®è§‚é¢„è­¦æ¨¡å— (Deep Macro)")
    if USER_FRED_KEY:
        try:
            f = Fred(api_key=USER_FRED_KEY)
            start = datetime.now() - timedelta(weeks=5)
            liq = (f.get_series('WALCL', observation_start=start).iloc[-1]/1e6) - \
                  (f.get_series('WTREGEN', observation_start=start).iloc[-1]/1e3) - \
                  (f.get_series('RRPONTSYD', observation_start=start).iloc[-1]/1e3)
            p_txt(f"1. ç¾è”å‚¨å‡€æµåŠ¨æ€§: ${liq:.3f}T")
            
            pe = scraper.fetch_shiller_pe() or 35.0
            erp = (1.0/pe*100) - f.get_series('DGS10', sort_order='desc', limit=1).iloc[-1]
            p_txt(f"2. è‚¡æƒé£é™©æº¢ä»· (ERP): {erp:.2f}%")
            
            try:
                d = yf.download(['SPY','RSP'], period="3mo", progress=False)['Close']
                chg = ((d['RSP'].iloc[-1]/d['SPY'].iloc[-1]) - (d['RSP'].iloc[-20]/d['SPY'].iloc[-20])) / (d['RSP'].iloc[-20]/d['SPY'].iloc[-20]) * 100
                p_txt(f"3. RSP/SPY ç›¸å¯¹å¼ºåº¦: {chg:+.2f}%")
            except: pass
        except: pass
    st.write("==================================================")

    # 8. Sector Rotation
    p_section("ğŸ”„ å¯åŠ¨æ¿å—è½®åŠ¨åˆ†ææ¨¡å—")
    secs = {'XLK':'ç§‘æŠ€','XLF':'é‡‘è','XLV':'åŒ»ç–—','XLE':'èƒ½æº','XLY':'å¯é€‰','XLP':'å¿…é€‰','XLI':'å·¥ä¸š','XLC':'é€šè®¯','XLB':'ææ–™','XLRE':'åœ°äº§','SPY':'åŸºå‡†'}
    d = get_cached_sector_data(list(secs.keys()), "2023-01-01")
    if not d.empty:
        c = d['Adj Close'] if 'Adj Close' in d else d['Close']
        rs = c.div(c['SPY'], axis=0); r = 100*(rs/rs.rolling(60).mean()); m = 100+((rs-rs.shift(10))/rs.shift(10)*100)
        
        p_txt("ğŸ“Š [RRG è±¡é™åˆ†å¸ƒ]")
        for q in ["Leading (é¢†æ¶¨)", "Weakening (è½¬å¼±)", "Lagging (è½å)", "Improving (æ”¹å–„)"]:
            l = []
            for t in secs:
                if t=='SPY': continue
                rv = r[t].iloc[-1]; mv = m[t].iloc[-1]
                if (rv>100 and mv>100 and "Leading" in q) or (rv<100 and mv<100 and "Lagging" in q) or (rv>100 and mv<100 and "Weakening" in q) or (rv<100 and mv>100 and "Improving" in q):
                    l.append(secs[t])
            if l: p_txt(f"   {q}: {', '.join(l)}")
        
        p_txt("ğŸš€ [10æ—¥ èµ„é‡‘æŠ¢ç­¹æ¦œ]")
        spy10 = (c['SPY'].iloc[-1]-c['SPY'].iloc[-11])/c['SPY'].iloc[-11]
        mov = []
        for t in secs:
            if t=='SPY': continue
            p = (c[t].iloc[-1]-c[t].iloc[-11])/c[t].iloc[-11]
            mov.append((secs[t], (p-spy10)*100))
        mov.sort(key=lambda x:x[1], reverse=True)
        for n, v in mov[:3]: p_txt(f"   ğŸ”¥ {n}: è·‘èµ¢å¤§ç›˜ {v:.2f}%")
    st.write("==================================================")

    # 9. SMT Analysis
    p_section("ğŸ§­ å¯åŠ¨ SMT èƒŒç¦»åˆ†ææ¨¡å— (Pro V3)")
    ts = ['^IXIC','^GSPC','QQQ','SPY','NQ=F','ES=F','RSP']
    d = get_cached_smt_data(ts, "6mo"); 
    if not d.empty:
        c = d['Close'].ffill()
        p_section("1. ç»å…¸ SMT åˆ†æ")
        for p in [3,5,10,20,60]:
            w = c.iloc[-(p+1):]; cur = w.iloc[-1]; h = w.max()
            nh = [t for t in ['^IXIC','^GSPC','QQQ','SPY'] if cur[t]>=h[t]*0.999]
            if len(nh)==4: p_txt(f"[{p}æ—¥çª—å£] ğŸ”¥ çŠ¶æ€: å¼ºå¤šå¤´å…±æŒ¯")
            elif len(nh)>0: p_txt(f"[{p}æ—¥çª—å£] âš ï¸ åˆ†æ­§: {nh} åˆ›æ–°é«˜")
        st.write("--------------------------------------------------")
        
        p_section("2. è¿›é˜¶ SMT åˆ†æ")
        w = c.iloc[-10:]; h = w.max(); cur = w.iloc[-1]
        if 'NQ=F' in w:
            nq_h = cur['NQ=F']>=h['NQ=F']*0.999; es_h = cur['ES=F']>=h['ES=F']*0.999
            if nq_h and not es_h: p_txt("ğŸ“Š [10æ—¥ æœŸè´§SMT]: ğŸ”´ [çœ‹è·Œ] çº³æŒ‡æ‹‰å‡ï¼Œæ ‡æ™®æ»æ¶¨")
            elif not nq_h and es_h: p_txt("ğŸ“Š [10æ—¥ æœŸè´§SMT]: ğŸ”´ [çœ‹è·Œ] æ ‡æ™®è¡¥æ¶¨ï¼Œç§‘æŠ€æ»æ¶¨")
            else: p_txt("ğŸ“Š [10æ—¥ æœŸè´§SMT]: ğŸŸ¢ æ­¥è°ƒä¸€è‡´")
        st.write("--------------------------------------------------")
        
        p_section("3. å…³é”®ä½ä¸å…¥åœºä¿¡å·")
        s = c['SPY']; ma20 = s.rolling(20).mean().iloc[-1]; now = s.iloc[-1]
        p_txt(f"ğŸ“Œ æ ‡æ™®ETF(SPY) ä»·æ ¼è¡Œä¸º:")
        p_txt(f"   ç°ä»·: {now:.2f} (MA20: {ma20:.2f})")
        if now > ma20: p_txt("   ğŸŒŠ [çŠ¶æ€]: è¶‹åŠ¿è¿è¡Œä¸­ (MA20ä¹‹ä¸Š)")
        else: p_txt("   â„ï¸ [ä¿¡å·]: è·Œç ´ MA20")
    st.write("==================================================")

    st.write("\n")
    p_ok(">>> è®¡ç®—å®Œæˆã€‚")
    st.stop()

if __name__ == "__main__":
    main()
