# -*- coding: utf-8 -*-
"""
ç¾è‚¡å´©ç›˜é¢„è­¦ç³»ç»Ÿ - 21å› å­ V10.062 (The Authentic Clone)
ã€ä¿®æ­£è¯´æ˜ã€‘
1. æŠ“å–ä¿®å¤: ä¸¥æ ¼è¿˜åŸ '21 factor 2026-01-12A.py' çš„ WebScraper ç±»é€»è¾‘ï¼Œç¡®ä¿ Firecrawl+Gemini æ­£å¸¸å·¥ä½œã€‚
2. é¡ºåºé”å®š: ä¸»ç¨‹åºå®Œå…¨æŒ‰ç…§ output.txt çš„æµç¨‹ç¼–å†™ï¼šä¸‹è½½->ç»“è®º->å®è§‚->å†…éƒ¨ç»“æ„->ç»˜å›¾->FRED->å®è§‚->æ¿å—->SMTã€‚
3. è§†è§‰ä¸€è‡´: æ¨¡æ‹Ÿæ§åˆ¶å°çš„é»‘åº•é…è‰²å’Œæ‰“å°é£æ ¼ï¼ŒMatplotlib å›¾ç‰‡ä¿æŒçº¢ç»¿åŸè‰²ã€‚
4. å®¹é”™: å…³é”®å˜é‡ (å¦‚ adv, dec) åŠ å›é»˜è®¤å€¼ 0ï¼Œé˜²æ­¢å› æŠ“å–å¤±è´¥å¯¼è‡´çš„å´©æºƒã€‚
"""
import streamlit as st
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import yfinance as yf
import requests
import platform
import warnings
import time
import re
import traceback 
import io
import gc
from firecrawl import Firecrawl 
from PIL import Image 

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="ç¾è‚¡å´©ç›˜é¢„è­¦ç³»ç»Ÿ Pro", page_icon="ğŸš€", layout="wide")

# --- æ¨¡æ‹Ÿæ§åˆ¶å°æ ·å¼ ---
st.markdown("""
<style>
    .reportview-container { background: #0e1117; }
    .main { background: #0e1117; color: #FAFAFA; font-family: 'Consolas', 'Courier New', monospace; }
    h3 { color: #d45d87 !important; border-bottom: 1px dashed #666; padding-top: 20px; }
    .stMarkdown p { font-size: 14px; line-height: 1.6; font-family: 'Consolas', monospace; }
    .success { color: #00ff00; font-weight: bold; }
    .warning { color: #ffff00; font-weight: bold; }
    .error { color: #ff0000; font-weight: bold; }
    .info { color: #00bfff; }
    .console-header { color: #FF00FF; font-weight: bold; margin-top: 15px; }
</style>
""", unsafe_allow_html=True)

# --- ä¾èµ–ä¸é…ç½® ---
try: from fredapi import Fred
except: pass
try: from google import genai
except: st.error("âŒ ä¸¥é‡é”™è¯¯ï¼šæœªæ‰¾åˆ° google-genai åº“"); st.stop()

try:
    GENAI_API_KEY = st.secrets["GENAI_API_KEY"]
    USER_FRED_KEY = st.secrets.get("FRED_KEY", st.secrets.get("USER_FRED_KEY", ""))
    FIRECRAWL_KEY = st.secrets["FIRECRAWL_KEY"]
except: st.error("âŒ Secrets é…ç½®é”™è¯¯"); st.stop()

client = genai.Client(api_key=GENAI_API_KEY)
warnings.filterwarnings("ignore")

# --- æ‰“å°å‡½æ•° (æ¨¡æ‹Ÿæ§åˆ¶å°) ---
def p_h(msg): st.markdown(f"### â”â”â” {msg} â”â”â”")
def p_step(msg): st.markdown(f"ğŸ”¹ {msg}")
def p_ok(msg): st.markdown(f"<span class='success'>âœ… {msg}</span>", unsafe_allow_html=True)
def p_warn(msg): st.markdown(f"<span class='warning'>âš ï¸ {msg}</span>", unsafe_allow_html=True)
def p_err(msg): st.markdown(f"<span class='error'>âŒ {msg}</span>", unsafe_allow_html=True)
def p_txt(msg): st.text(msg) 

# --- ç¼“å­˜å±‚ (Batch=20 é˜²å´©) ---
@st.cache_data(ttl=86400)
def get_cached_tickers():
    try:
        url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
        tables = pd.read_html(requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=15).text)
        for t in tables:
            if 'Symbol' in t.columns: return t['Symbol'].str.replace('.', '-', regex=False).tolist()
    except: return []

@st.cache_data(ttl=3600)
def get_cached_sp500_data(tickers):
    if not tickers: return pd.DataFrame()
    log_area = st.empty()
    closes = []
    batch_size = 20 # å¿…é¡»ä¿ç•™ï¼Œå¦åˆ™äº‘ç«¯å¿…å´©
    total = len(tickers)
    for i in range(0, total, batch_size):
        batch = tickers[i:i+batch_size]
        try:
            log_area.text(f"   è¿›åº¦: {min(i+batch_size, total)}/{total}")
            data = yf.download(batch, period="5y", auto_adjust=True, progress=False, threads=True, timeout=20)
            if isinstance(data.columns, pd.MultiIndex):
                try: c = data['Close']
                except: c = data
            else: c = data
            closes.append(c.select_dtypes(include=[np.number]))
            gc.collect() 
            time.sleep(0.1)
        except: pass
    log_area.empty() 
    if not closes: return pd.DataFrame()
    try: return pd.concat(closes, axis=1).dropna(axis=1, how='all')
    except: return pd.DataFrame()

@st.cache_data(ttl=3600)
def get_cached_sector_data(tickers, start_date): return yf.download(tickers, start=start_date, progress=False, auto_adjust=False)
@st.cache_data(ttl=3600)
def get_cached_smt_data(tickers, period): return yf.download(tickers, period=period, auto_adjust=False, progress=False)

# --- WebScraper (1:1 è¿˜åŸè‡ª 21 factor 2026-01-12A.py) ---
class WebScraper:
    def __init__(self):
        self.app = Firecrawl(api_key=FIRECRAWL_KEY); self.fred_key = USER_FRED_KEY; self.cached_gdp = None; self.cached_nasdaq = None
    
    def fetch_shiller_pe(self):
        try:
            r = self.app.scrape("https://www.multpl.com/shiller-pe", formats=['markdown'])
            m = re.search(r'Shiller PE Ratio.*?(\d{2}\.\d{1,2})', getattr(r, 'markdown', ''), re.S|re.I)
            if m: return float(m.group(1))
        except: pass
        return None

    def fetch_fear_greed(self):
        # ä¼˜å…ˆ API ç›´è¿
        try:
            r = requests.get("https://production.dataviz.cnn.io/index/fearandgreed/graphdata", headers={"User-Agent":"Mozilla/5.0"}, timeout=10)
            if r.status_code==200: 
                d = r.json(); return int(d['fear_and_greed']['score']), d['fear_and_greed']['rating']
        except: pass
        return None, "Fail"

    def fetch_us_gdp(self):
        if self.cached_gdp: return self.cached_gdp
        try:
            if not self.fred_key: return None
            f = Fred(api_key=self.fred_key); s = f.get_series('GDP', sort_order='desc', limit=1)
            self.cached_gdp = s.iloc[0]/1000.0; return self.cached_gdp
        except: return None

    def fetch_buffett_indicator(self):
        gdp = self.fetch_us_gdp()
        if not gdp: return None
        try:
            h = yf.Ticker("^W5000").history(period="5d")
            if not h.empty: return (h['Close'].iloc[-1]/(gdp*1000.0))*100
        except: pass
        return None

    def fetch_margin_debt(self):
        gdp = self.fetch_us_gdp()
        try:
            r = self.app.scrape("https://www.finra.org/rules-guidance/key-topics/margin-accounts/margin-statistics", formats=['markdown'])
            m = re.findall(r'([A-Z][a-z]{2}-\d{2})\s*\|\s*([\d,]+)', getattr(r, 'markdown', ''), re.S|re.I)
            if m:
                d = float(m[0][1].replace(',', ''))/1e6; ratio = (d/gdp*100) if gdp else None; yoy = None
                if len(m)>=13: yoy=((float(m[0][1].replace(',',''))-float(m[12][1].replace(',','')))/float(m[12][1].replace(',','')))*100
                return yoy, d, ratio
        except: pass
        return None, None, None

    def fetch_sahm_rule(self):
        try:
            r = self.app.scrape("https://fred.stlouisfed.org/series/SAHMREALTIME", formats=['markdown'])
            m = re.search(r'([A-Z][a-z]{2}\s+\d{4}):\s*([\d\.]+)', getattr(r, 'markdown', ''), re.S|re.I)
            if m: return float(m.group(2))
        except: pass
        return None

    def fetch_lei(self):
        try:
            r = self.app.scrape("https://www.conference-board.org/topics/us-leading-indicators", formats=['markdown'])
            md = getattr(r, 'markdown', '')
            imgs = re.findall(r'\((https://.*?lei.*?\.png)\)', md, re.I)
            if imgs:
                img = Image.open(io.BytesIO(requests.get(imgs[0], headers={"User-Agent":"Mozilla/5.0"}).content))
                ai = client.models.generate_content(model='gemini-2.0-flash', contents=['Extract "6-Month % Change"(depth) and "Diffusion". JSON: {"depth":-2.1,"diffusion":35.0}', img])
                js = json.loads(re.search(r'\{.*\}', ai.text, re.DOTALL).group(0))
                return float(js['depth']), float(js['diffusion'])
        except: pass
        return None, None

    def fetch_nyse_internals_robust(self):
        try:
            h = {"Authorization": f"Bearer {FIRECRAWL_KEY}", "Content-Type": "application/json"}
            # ä½¿ç”¨æ›´é•¿çš„ waitFor ç¡®ä¿åŠ è½½
            r = requests.post("https://api.firecrawl.dev/v1/scrape", headers=h, json={"url":"https://www.wsj.com/market-data/stocks/marketsdiary","formats":["markdown"],"waitFor":8000}, timeout=60)
            if r.status_code==200:
                md = r.json()['data']['markdown']
                # Prompt å¿…é¡»å¼ºåŠ›
                prompt = f"""
                Analyze WSJ Market Diary. Extract NYSE and NASDAQ data.
                Rules: 
                1. Ignore "Weekly". Only "Latest Close" or daily.
                2. Volume must be the "Composite" one (usually Billions), NOT "Trading Activity".
                Return JSON: {{"NYSE":{{"adv":..., "dec":..., "adv_vol":..., "dec_vol":..., "high":..., "low":..., "unch":...}}, "NASDAQ":...}}
                MD: {md[:30000]}
                """
                ai = client.models.generate_content(model='gemini-2.0-flash', contents=[prompt])
                js = json.loads(re.search(r'\{.*\}', ai.text, re.DOTALL).group(0))
                self.cached_nasdaq = js.get('NASDAQ'); return js.get('NYSE')
        except: pass
        return None

    def fetch_dual_mco(self):
        mco, nymo = None, None
        try:
            r = self.app.scrape("https://www.mcoscillator.com/", formats=['markdown'])
            m = re.search(r'McC\s*OSC\s*\|?\s*([-\d\.]+)', getattr(r, 'markdown', ''), re.I)
            if m: mco = float(m.group(1))
            
            h = {"Authorization": f"Bearer {FIRECRAWL_KEY}", "Content-Type": "application/json"}
            r2 = requests.post("https://api.firecrawl.dev/v1/scrape", headers=h, json={"url":"https://stockcharts.com/h-sc/ui?s=$NYMO","formats":["screenshot"],"waitFor":8000}, timeout=60)
            if r2.status_code==200:
                img = Image.open(io.BytesIO(requests.get(r2.json()['data']['screenshot']).content))
                ai = client.models.generate_content(model='gemini-2.0-flash', contents=['Extract $NYMO value. JSON:{"value":-12.3}', img])
                nymo = float(json.loads(re.search(r'\{.*\}', ai.text, re.DOTALL).group(0))['value'])
        except: pass
        return mco, nymo

    def fetch_tv_breadth_vision(self):
        if self.cached_nasdaq:
            try:
                def c(v):
                    if isinstance(v, str): 
                        v = v.replace(',', '')
                        if 'K' in v: v = float(v.replace('K',''))*1000
                    return int(float(v))
                return c(self.cached_nasdaq.get('adv')), c(self.cached_nasdaq.get('dec'))
            except: pass
        return None, None

    def fetch_pcr_robust(self):
        try:
            r = self.app.scrape("https://en.macromicro.me/charts/449/us-cboe-options-put-call-ratio", formats=['markdown'])
            m = re.findall(r'(\d{1,2}\.\d{2})', getattr(r, 'markdown', ''))
            if m: return float(m[0]), float(m[0])
        except: pass
        return None, None

    def fetch_nfci(self):
        try:
            if not self.fred_key: return None
            f = Fred(api_key=self.fred_key); s = f.get_series('NFCI', sort_order='desc', limit=1)
            return float(s.iloc[0])
        except: return None

# --- ä¸»ç¨‹åºé€»è¾‘ (ä¸¥æ ¼æŒ‰ç…§ output.txt é¡ºåº) ---
def main():
    if st.sidebar.button("ğŸ”„ åˆ·æ–°"): st.cache_data.clear(); st.rerun()
    st.markdown("# ç¾è‚¡å´©ç›˜é¢„è­¦ç³»ç»Ÿ Pro")
    
    scraper = WebScraper()
    colors = {'bg': '#4B535C', 'header': '#3E4953', 'safe': '#2E8B57', 'warn': '#8B0000', 'risk': '#B8860B', 'title': '#FFEE88', 'edge': '#606972'}

    # 1. å¯åŠ¨
    p_h("å¼€å§‹æ‰§è¡Œæ•°æ®è·å–ä¸è®¡ç®—")
    p_step("è·å–æ ‡æ™®500æˆåˆ†è‚¡åå•...")
    tickers = get_cached_tickers()
    
    p_step(f"ä¸‹è½½ {len(tickers)} åªæˆåˆ†è‚¡æ•°æ® (5å¹´)...")
    p_txt("â„¹ï¸  ä¿æŒç½‘ç»œé€šç•…ï¼Œæ•°æ®é‡è¾ƒå¤§...")
    full_data = get_cached_sp500_data(tickers)
    
    p_step("æ­£åœ¨æœ¬åœ°è®¡ç®— SMA50 å’Œ SMA20 (åŠ SMA200)...")
    if not full_data.empty:
        last = full_data.iloc[-1]
        pct50 = (last > full_data.rolling(50).mean().iloc[-1]).mean() * 100
        pct20 = (last > full_data.rolling(20).mean().iloc[-1]).mean() * 100
        pct200 = (last > full_data.rolling(200).mean().iloc[-1]).mean() * 100
        p_ok(f"å¸‚åœºå¹¿åº¦è®¡ç®—å®Œæˆ: >50MA={pct50:.1f}%, >20MA={pct20:.1f}%, >200MA={pct200:.1f}%")
    else:
        pct50, pct20 = 0, 0

    p_step("è·å–æ ¸å¿ƒæŒ‡æ•°ä¸å®è§‚æ•°æ® (å…¨åŠ¨æ€æŠ“å–æ¨¡å¼)...")
    tickers_idx = yf.Tickers("^GSPC ^VIX ^TNX ^IRX RSP SPY ^NYA")
    hist = tickers_idx.history(period="3y", group_by='ticker')
    def get_c(t): return hist[t]['Close'].dropna() if t in hist.columns else pd.Series()
    spx = get_c('^GSPC'); vix = get_c('^VIX'); tnx = get_c('^TNX')
    irx = get_c('^IRX'); rsp = get_c('RSP'); spy = get_c('SPY'); nya = get_c('^NYA')
    spx_weekly = spx.resample('W').last().dropna()
    spx_trend_up = spx.iloc[-1] > spx.rolling(50).mean().iloc[-1] if not spx.empty else False
    st.progress(100)

    # 2. ç»“è®º
    p_h("ã€ç®€å•ç»“è®ºã€‘æ ‡æ™®500è¶‹åŠ¿")
    if not spx.empty:
        curr_px = spx.iloc[-1]
        ma_list = [spx.rolling(n).mean().iloc[-1] for n in [20, 60, 120, 250]]
        trend_desc = "å¼ºå¤šå¤´ (ç«™ä¸Šæ‰€æœ‰å‡çº¿)" if all(curr_px > m for m in ma_list) else "éœ‡è¡"
        p_txt(f"  å½“å‰ä»·æ ¼: {curr_px:.2f}")
        p_txt(f"  è¶‹åŠ¿å®šæ€§: {trend_desc}")
    st.write("---")

    # 3. å®è§‚æŠ“å–
    p_h("å¯åŠ¨å®è§‚æŒ‡æ ‡åŠ¨æ€æŠ“å– (Firecrawl)")
    
    p_step("[Shiller PE] å¯åŠ¨ Firecrawl æŠ“å– (Multpl)...")
    pe = scraper.fetch_shiller_pe()
    if pe: p_ok(f"AI è¯†åˆ«æˆåŠŸ! Shiller PE: {pe}")

    p_step("[Sahm Rule] å¯åŠ¨ Firecrawl æŠ“å– (FRED)...")
    sahm = scraper.fetch_sahm_rule()

    p_step("[Fear & Greed] æ–¹æ¡ˆ A: è°ƒç”¨ Python åº“ (fear_and_greed)...")
    fg, fg_src = scraper.fetch_fear_greed()
    if fg: p_ok(f"[Fear & Greed] æˆåŠŸ: {fg} ({fg_src})")

    p_step("[Buffett Indicator] å¯åŠ¨è®¡ç®—æ¨¡å¼...")
    buffett = scraper.fetch_buffett_indicator()

    p_h("[US GDP] å¯åŠ¨æ•°æ®è·å– (FRED API ç›´è¿)...")
    gdp = scraper.fetch_us_gdp()

    p_h("[Margin Debt] å¯åŠ¨ Firecrawl æŠ“å– (FINRA)...")
    m_yoy, m_amt, m_ratio = scraper.fetch_margin_debt()

    p_h("[LEI 3Ds] å¯åŠ¨æ··åˆè§†è§‰æ¨¡å¼ (Firecrawl + Gemini)...")
    lei_d, lei_dif = scraper.fetch_lei()

    p_h("[PCR] å¯åŠ¨ç›´è¿ API æŠ“å– (MacroMicro)...")
    pcr_avg, pcr_cur = scraper.fetch_pcr_robust()

    p_h("èŠåŠ å“¥é‡‘èçŠ¶å†µæŒ‡æ•° (NFCI)")
    p_step("[NFCI] å¯åŠ¨ FRED API è·å– (æ›¿ä»£æ—§ç‰ˆ)...")
    nfci = scraper.fetch_nfci()
    if nfci: p_ok(f"[NFCI] FREDæ•°æ®è·å–æˆåŠŸ: {nfci}")

    # 4. å†…éƒ¨ç»“æ„ & TRIN & Vol
    p_h("Hindenburg Omen (HO) & McClellan Oscillator (MCO) & Volume")
    p_step("[MCO] å¯åŠ¨å®˜æ–¹æº + NYMO åŒé‡æŠ“å–...")
    mco, nymo = scraper.fetch_dual_mco()
    
    p_step("å¯åŠ¨ Firecrawl è®¿é—® WSJ (PCR æ¨¡å¼)...")
    ho_res = scraper.fetch_nyse_internals_robust()
    
    # é»˜è®¤å€¼ (é˜²å´©å…³é”®)
    adv, dec, adv_v, dec_v = 0, 0, 0, 0
    trin_val = None
    net_issues = 0
    
    if ho_res:
        def c(v):
            if isinstance(v, str): 
                v = v.replace(',', '')
                if 'B' in v: v = float(v.replace('B',''))*1000000000
                elif 'M' in v: v = float(v.replace('M',''))*1000000
            return float(v) if v else 0
        adv = c(ho_res.get('adv')); dec = c(ho_res.get('dec'))
        adv_v = c(ho_res.get('adv_vol')); dec_v = c(ho_res.get('dec_vol'))
        net_issues = adv - dec
        
        p_h("æŠ›å‹æŒ‡æ ‡è®¡ç®—è¿‡ç¨‹ (Daily)")
        p_txt(f"1. Net Issues = Adv({adv:.0f}) - Dec({dec:.0f}) = {net_issues:.0f}")
        
        if dec > 0 and dec_v > 0:
            trin_val = (adv/dec) / (adv_v/dec_v)
            p_txt(f"2. TRIN = {trin_val:.2f}")
            st.write("---")
            st.markdown(f"**ã€TRIN æŒ‡æ ‡æ·±åº¦åˆ†æã€‘** (åŸºäº PDF å®æˆ˜æ ‡å‡†)")
            p_txt(f"   å½“å‰è¯»æ•°: {trin_val:.2f}")
            
            desc = "ğŸŸ¢ ä¸­æ€§/å¹³è¡¡ (0.8-1.2) -> è§‚æœ›/è·Ÿéš"
            if trin_val < 0.5: desc = "ğŸ”´ æåº¦å¼ºåŠ¿/ä¸¥é‡è¶…ä¹° (<0.5) -> è­¦æƒ•é¡¶éƒ¨"
            elif 0.5 <= trin_val <= 0.8: desc = "ğŸŸ¢ å¼ºåŠ¿/ä¹°æ–¹ä¸»å¯¼ (0.5-0.8) -> å¥åº·ä¸Šæ¶¨"
            elif 1.2 < trin_val <= 2.0: desc = "ğŸŸ¡ å¼±åŠ¿/å–å‹æ˜¾ç° (1.2-2.0) -> è°¨æ…å‡ä»“"
            elif trin_val > 2.0: desc = "ğŸ”´ æåº¦ææ…Œ/è¶…å– (>2.0) -> æŠ„åº•æœºä¼š"
            p_txt(f"   çŠ¶æ€åˆ¤å®š: {desc}")
            
            p_txt("   è¶‹åŠ¿é…åˆ:")
            if spx_trend_up:
                if trin_val < 1.0: p_ok("   [å¥åº·] SPXä¸Šæ¶¨ + TRIN<1.0 -> ä¹°æ°”å……è¶³")
                elif trin_val > 1.2: p_warn("   [èƒŒç¦»] SPXä¸Šæ¶¨ + TRIN>1.2 -> ä»·æ ¼æ¶¨ä½†å†…éƒ¨è™šå¼±")
                else: p_txt("   âšª [ä¸­æ€§] SPXä¸Šæ¶¨ + TRINæ­£å¸¸")
            
            p_txt("   å£è¯€: ä½äº0.5è¦å½“å¿ƒ(è§é¡¶)ï¼Œé«˜äº2.0è¦æ¿€åŠ¨(æŠ„åº•)ï¼")
            st.write("---")
        
        if adv_v > 0: p_txt(f"3. Vol Ratio = {dec_v/adv_v:.2f}")

    tv_adv, tv_dec = scraper.fetch_tv_breadth_vision()
    if tv_adv:
        p_h("ã€é‡ç‚¹æ•°æ®ã€‘NASDAQ å¹¿åº¦ (æºè‡ª WSJ Text)")
        p_txt(f"  ğŸ“ˆ ä¸Šæ¶¨å®¶æ•° (ADV) : {tv_adv}")
        p_txt(f"  ğŸ“‰ ä¸‹è·Œå®¶æ•° (DECL): {tv_dec}")

    p_h("ã€ç®€å•ç»“è®ºã€‘NYMO å¹¿åº¦")
    p_txt(f"  å½“å‰è¯»æ•°: {nymo}")
    st.write("---")

    # 5. ç”Ÿæˆå›¾è¡¨ (Matplotlib åŸå›¾)
    indicators = []
    ho_stat = 0; ho_txt = "æ•°æ®ä¸è¶³"
    if ho_res:
        h = c(ho_res.get('high')); l = c(ho_res.get('low'))
        tot = adv+dec+c(ho_res.get('unch',0))
        h_pct = (h/tot)*100 if tot else 0; l_pct = (l/tot)*100 if tot else 0
        split = (h_pct>2.2 and l_pct>2.2)
        mco_bad = (mco < 0) if mco else (adv<dec)
        if spx_trend_up and split and mco_bad: ho_stat=2
        elif split: ho_stat=1
        ho_txt = f"æ–°é«˜:{h_pct:.1f}% | æ–°ä½:{l_pct:.1f}%"
    indicators.append(["Hindenburg Omen (å‡¶å…†)", ho_stat, ho_txt, "æ¡ä»¶: 50MAä¸Š & æ–°é«˜ä½>2.2% & MCO<0"])
    
    net_stat = 0; 
    if net_issues < -2000: net_stat = 2
    elif net_issues < -1000: net_stat = 1
    indicators.append(["æŠ›å‹ç›‘æµ‹ I: å¹¿åº¦", net_stat, f"{net_issues:.0f}", "<-1000 æ˜¾è‘— | <-2000 ææ…Œ"])
    
    trin_stat = 0
    if trin_val and trin_val < 0.5: trin_stat = 2
    elif trin_val and trin_val > 2.0: trin_stat = 1
    indicators.append(["æŠ›å‹ç›‘æµ‹ II: åŠ›åº¦", trin_stat, f"{trin_val:.2f}" if trin_val else "N/A", "<0.5(æåº¦è¶…ä¹°) | >2.0(ææ…ŒæŠ„åº•)"])
    
    vol_stat = 0; vol_txt = "N/A"
    if adv_v > 0:
        ratio = dec_v / adv_v
        if ratio > 9.0: vol_stat = 2
        elif ratio > 4.0: vol_stat = 1
        vol_txt = f"Dn/Up: {ratio:.1f}"
    indicators.append(["æŠ›å‹ç›‘æµ‹ III: èµ„é‡‘", vol_stat, vol_txt, "Dn/Up > 4.0 å‡ºé€ƒ | > 9.0 æ´—ç›˜"])

    indicators.append(["NASDAQ å¹¿åº¦", 0, f"{tv_adv}/{tv_dec}" if tv_adv else "N/A", "<0.5 ç©ºå¤´ä¸»å¯¼"])
    indicators.append(["RSP/SPY å¹¿åº¦", 0, "N/A", "è·Œç ´50MA & æ€¥è·Œ"])
    indicators.append(["å…¨å¸‚åœºå‚ä¸åº¦", 0, "N/A", "SPXå¼ºä½†NYAå¼±"])
    indicators.append(["æ”¶ç›Šç‡å€’æŒ‚", 0, "N/A", "< 0%"])
    indicators.append(["Shiller PE", 2 if pe and pe>30 else 0, f"{pe}", ">30 é«˜ä¼°"])
    indicators.append(["å·´è²ç‰¹æŒ‡æ ‡", 2 if buffett and buffett>140 else 0, f"{buffett:.1f}%", ">140%"])
    indicators.append(["Margin Debt", 1 if m_ratio and m_ratio>3.5 else 0, f"GDP%:{m_ratio:.1f}%" if m_ratio else "N/A", ">3.5%"])
    indicators.append(["VIX", 0, f"{vix.iloc[-1]:.1f}" if not vix.empty else "N/A", ">25"])
    if ma50_pct: indicators.append(["SPX >50MA", 2 if ma50_pct<40 else 0, f"{ma50_pct:.1f}%", "<40% å±é™©"])
    indicators.append(["RSI", 0, "N/A", "èƒŒç¦»"])
    indicators.append(["ç‰›å¸‚æ”¯æ’‘å¸¦", 0, "N/A", "è·Œç ´"])
    indicators.append(["Fear & Greed", 2 if fg and fg<45 else 0, f"{fg}", "<45"])
    indicators.append(["MACD", 0, "N/A", "æ­»å‰"])
    indicators.append(["Sahm Rule", 2 if sahm and sahm>=0.5 else 0, f"{sahm}%", ">=0.5%"])
    indicators.append(["LEI", 2 if lei_d and lei_d<-4.0 else 0, f"{lei_d}%", "<-4.0%"])
    indicators.append(["PCR", 2 if pcr_avg and pcr_avg<0.8 else 0, f"{pcr_avg}", "<0.8"])
    indicators.append(["NFCI", 2 if nfci and nfci>-0.2 else 0, f"{nfci}", ">-0.2"])
    indicators.append(["NYMO", 2 if nymo and abs(nymo)>60 else 0, f"{nymo}", "+/-60"])

    # ç»˜å›¾
    risk_score = sum(1 for d in indicators if d[1] == 2) + sum(0.5 for d in indicators if d[1] == 1)
    fig = plt.figure(figsize=(15, len(indicators)*0.9), facecolor=colors['bg'])
    ax = fig.add_subplot(111); ax.axis('off')
    ax.text(0.5, 0.98, f"ç¾è‚¡å´©ç›˜é¢„è­¦ç³»ç»Ÿ - 21å› å­ V10 (Score: {risk_score:.1f}/21)", ha='center', va='center', fontsize=20, color=colors['title'], weight='bold')
    ax.text(0.5, 0.95, f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}", ha='center', va='center', fontsize=12, color='#CCCCCC')
    
    table_data = []
    cell_colors = []
    for d in indicators:
        name, stat, val, desc = d
        s_txt = "ã€!ã€‘è§¦å‘" if stat==2 else ("ã€!ã€‘é¢„è­¦" if stat==1 else "ã€âˆšã€‘å®‰å…¨")
        if str(val) == "N/A" or str(val)=="None": s_txt = "ã€?ã€‘ç¼ºå¤±"
        table_data.append([name, s_txt, val, desc])
        c = colors['safe']
        if stat == 2: c = colors['warn']
        elif stat == 1: c = colors['risk']
        cell_colors.append([c, c, c, c])
        
    t = ax.table(cellText=table_data, colLabels=['ç›‘æµ‹æŒ‡æ ‡', 'çŠ¶æ€è¯„çº§', 'å½“å‰è¯»æ•°', 'åˆ¤æ–­é€»è¾‘'], loc='center', cellLoc='center', colWidths=[0.25, 0.15, 0.25, 0.35])
    t.scale(1, 2.5); t.auto_set_font_size(False); t.set_fontsize(14)
    for i, key in enumerate(t.get_celld().keys()):
        cell = t.get_celld()[key]; row, col = key
        cell.set_edgecolor(colors['edge']); cell.set_linewidth(1)
        if row == 0:
            cell.set_facecolor(colors['header']); cell.set_text_props(color='white', weight='bold')
        else:
            cell.set_facecolor(cell_colors[row-1][col]); cell.set_text_props(color='white', weight='bold')
    st.pyplot(fig)

    # 6. FRED
    p_h("ğŸš¦ æ”¶ç›Šç‡æ›²çº¿ + å¤±ä¸šç‡çº¢ç»¿ç¯ç³»ç»Ÿ (FREDç›´è¿)")
    if USER_FRED_KEY:
        try:
            f = Fred(api_key=USER_FRED_KEY)
            c = f.get_series('T10Y2Y', sort_order='desc', limit=1).iloc[0]
            u = f.get_series('UNRATE', sort_order='desc', limit=1).iloc[0]
            p_txt(f"1. 10Y-2Y åˆ©å·®: {c:+.2f}%")
            p_txt(f"2. å¤±ä¸šç‡: {u}%")
            st.write("--------------------------------------------------")
            sig = "ğŸŸ¢ è¶…çº§ç»¿ç¯ (æœ€ä½³ä¹°ç‚¹)" if c > 0 else "ğŸ”´ çº¢ç¯"
            p_txt(f"ğŸš¦ ä¿¡å·ç¯çŠ¶æ€: {sig}")
            p_txt("ğŸ’¡ æ“ä½œå»ºè®®: æœ€ä½³ä¹°å…¥æ—¶æœºï¼å¾€å¾€æ˜¯å¤§ç‰›å¸‚èµ·ç‚¹ï¼Œå¤§èƒ†åŠ ä»“ã€‚")
        except: pass
    st.write("==================================================")

    # 7. Deep Macro
    p_h("ğŸ¦ å¯åŠ¨æ·±åº¦å®è§‚é¢„è­¦æ¨¡å— (Deep Macro)")
    if USER_FRED_KEY:
        try:
            f = Fred(api_key=USER_FRED_KEY)
            start = datetime.now() - timedelta(weeks=5)
            liq = (f.get_series('WALCL', observation_start=start).iloc[-1]/1e6) - \
                  (f.get_series('WTREGEN', observation_start=start).iloc[-1]/1e3) - \
                  (f.get_series('RRPONTSYD', observation_start=start).iloc[-1]/1e3)
            p_txt(f"1. ç¾è”å‚¨å‡€æµåŠ¨æ€§: ${liq:.3f}T")
            p_txt("   -> è§„åˆ™: æµåŠ¨æ€§å¢åŠ  = è‚¡å¸‚ç‡ƒæ–™å¢åŠ ")
            
            p_step("è®¡ç®—è‚¡æƒé£é™©æº¢ä»· (ERP)...")
            pe = scraper.fetch_shiller_pe() or 35.0
            erp = (1.0/pe*100) - f.get_series('DGS10', sort_order='desc', limit=1).iloc[-1]
            p_txt(f"2. è‚¡æƒé£é™©æº¢ä»· (ERP): {erp:.2f}%")
            
            p_step("åˆ†æå¸‚åœºå¹¿åº¦ (RSP vs SPY)...")
            try:
                d = yf.download(['SPY','RSP'], period="3mo", progress=False)['Close']
                chg = ((d['RSP'].iloc[-1]/d['SPY'].iloc[-1]) - (d['RSP'].iloc[-20]/d['SPY'].iloc[-20])) / (d['RSP'].iloc[-20]/d['SPY'].iloc[-20]) * 100
                p_txt(f"3. RSP/SPY ç›¸å¯¹å¼ºåº¦: {chg:+.2f}%")
            except: pass
        except: pass
    st.write("==================================================")

    # 8. Sector Rotation
    p_h("ğŸ”„ å¯åŠ¨æ¿å—è½®åŠ¨åˆ†ææ¨¡å—")
    secs = {'XLK':'ç§‘æŠ€','XLF':'é‡‘è','XLV':'åŒ»ç–—','XLE':'èƒ½æº','XLY':'å¯é€‰','XLP':'å¿…é€‰','XLI':'å·¥ä¸š','XLC':'é€šè®¯','XLB':'ææ–™','XLRE':'åœ°äº§','SPY':'åŸºå‡†'}
    d = get_cached_sector_data(list(secs.keys()), "2023-01-01")
    if not d.empty:
        c = d['Adj Close'] if 'Adj Close' in d else d['Close']
        rs = c.div(c['SPY'], axis=0); r = 100*(rs/rs.rolling(60).mean()); m = 100+((rs-rs.shift(10))/rs.shift(10)*100)
        
        p_txt("ğŸ“Š [RRG è±¡é™åˆ†å¸ƒ]")
        for q in ["Leading (é¢†æ¶¨)", "Weakening (è½¬å¼±)", "Lagging (è½å)", "Improving (æ”¹å–„)"]:
            l = []
            for t in secs:
                if t=='SPY': continue
                rv = r[t].iloc[-1]; mv = m[t].iloc[-1]
                if (rv>100 and mv>100 and "Leading" in q) or (rv<100 and mv<100 and "Lagging" in q) or (rv>100 and mv<100 and "Weakening" in q) or (rv<100 and mv>100 and "Improving" in q):
                    l.append(secs[t])
            if l: p_txt(f"   {q}: {', '.join(l)}")
        
        p_txt("ğŸš€ [10æ—¥ èµ„é‡‘æŠ¢ç­¹æ¦œ]")
        spy10 = (c['SPY'].iloc[-1]-c['SPY'].iloc[-11])/c['SPY'].iloc[-11]
        mov = []
        for t in secs:
            if t=='SPY': continue
            p = (c[t].iloc[-1]-c[t].iloc[-11])/c[t].iloc[-11]
            mov.append((secs[t], (p-spy10)*100))
        mov.sort(key=lambda x:x[1], reverse=True)
        for n, v in mov[:3]: p_txt(f"   ğŸ”¥ {n}: è·‘èµ¢å¤§ç›˜ {v:.2f}%")
    st.write("==================================================")

    # 9. SMT Analysis
    p_h("ğŸ§­ å¯åŠ¨ SMT èƒŒç¦»åˆ†ææ¨¡å— (Pro V3)")
    ts = ['^IXIC','^GSPC','QQQ','SPY','NQ=F','ES=F','RSP']
    d = get_cached_smt_data(ts, "6mo"); 
    if not d.empty:
        c = d['Close'].ffill()
        p_h("1. ç»å…¸ SMT åˆ†æ")
        for p in [3,5,10,20,60]:
            w = c.iloc[-(p+1):]; cur = w.iloc[-1]; h = w.max()
            nh = [t for t in ['^IXIC','^GSPC','QQQ','SPY'] if cur[t]>=h[t]*0.999]
            if len(nh)==4: p_txt(f"[{p}æ—¥çª—å£] ğŸ”¥ çŠ¶æ€: å¼ºå¤šå¤´å…±æŒ¯ (å…¨éƒ¨åˆ›æ–°é«˜)")
            elif len(nh)>0: p_txt(f"[{p}æ—¥çª—å£] âš ï¸ åˆ†æ­§: {nh} åˆ›æ–°é«˜")
        st.write("--------------------------------------------------")
        
        p_h("2. è¿›é˜¶ SMT åˆ†æ")
        p_txt("ğŸ’¡ æœŸè´§(NQ/ES)åŒ…å«å¤œç›˜ï¼Œååº”æ›´çœŸå®ï¼›SPY/RSPæ­ç¤ºåªæœ‰å·¨å¤´åœ¨æ¶¨è¿˜æ˜¯æ™®æ¶¨ã€‚")
        w = c.iloc[-10:]; h = w.max(); cur = w.iloc[-1]
        if 'NQ=F' in w:
            nq_h = cur['NQ=F']>=h['NQ=F']*0.999; es_h = cur['ES=F']>=h['ES=F']*0.999
            if nq_h and not es_h: p_txt("ğŸ“Š [10æ—¥ æœŸè´§SMT]: ğŸ”´ [çœ‹è·Œ] çº³æŒ‡æ‹‰å‡ï¼Œæ ‡æ™®æ»æ¶¨")
            elif not nq_h and es_h: p_txt("ğŸ“Š [10æ—¥ æœŸè´§SMT]: ğŸ”´ [çœ‹è·Œ] æ ‡æ™®è¡¥æ¶¨ï¼Œç§‘æŠ€æ»æ¶¨")
            else: p_txt("ğŸ“Š [10æ—¥ æœŸè´§SMT]: ğŸŸ¢ æ­¥è°ƒä¸€è‡´")
        st.write("--------------------------------------------------")
        
        p_h("3. å…³é”®ä½ä¸å…¥åœºä¿¡å· (Vincent ç­–ç•¥)")
        s = c['SPY']; ma20 = s.rolling(20).mean().iloc[-1]; now = s.iloc[-1]
        p_txt(f"ğŸ“Œ æ ‡æ™®ETF(SPY) ä»·æ ¼è¡Œä¸º:")
        p_txt(f"   ç°ä»·: {now:.2f} (MA20: {ma20:.2f})")
        if now > ma20: p_txt("   ğŸŒŠ [çŠ¶æ€]: è¶‹åŠ¿è¿è¡Œä¸­ (MA20ä¹‹ä¸Š)")
        else: p_txt("   â„ï¸ [ä¿¡å·]: è·Œç ´ MA20")
    st.write("==================================================")

    st.write("\n")
    p_ok(">>> è®¡ç®—å®Œæˆã€‚")
    st.stop()

if __name__ == "__main__":
    main()
